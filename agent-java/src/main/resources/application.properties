# Server Configuration
server.port=10003

# Application Name
spring.application.name=aem-a2ui-agent

# JSON Output
spring.jackson.serialization.indent-output=true

# Embabel Agent Configuration
embabel.agent.platform.name=aem-content-assistant
embabel.agent.platform.description=AI-powered content assistant for AEM authoring
embabel.agent.platform.scanning.annotation=true

# LLM Provider Configuration (uses OPENAI_API_KEY or ANTHROPIC_API_KEY env vars)
embabel.agent.platform.models.openai.max-attempts=3
embabel.agent.platform.models.openai.backoff-millis=1000
embabel.agent.platform.models.anthropic.max-attempts=3
embabel.agent.platform.models.anthropic.backoff-millis=1000

# Autonomy Settings
embabel.agent.platform.autonomy.agent-confidence-cut-off=0.6
embabel.agent.platform.autonomy.goal-confidence-cut-off=0.6

# LLM Operations
embabel.llm-operations.prompts.generate-examples-by-default=true

# Feature flags for AI mode
aem.agent.ai.enabled=${AI_ENABLED:false}
aem.agent.ai.fallback-to-templates=true

# LLM Provider Selection: openai, anthropic, or ollama
aem.agent.llm.provider=${LLM_PROVIDER:ollama}

# OpenAI Configuration
aem.agent.llm.openai.base-url=https://api.openai.com/v1
aem.agent.llm.openai.model=${OPENAI_MODEL:gpt-4o-mini}

# Anthropic Configuration
aem.agent.llm.anthropic.base-url=https://api.anthropic.com/v1
aem.agent.llm.anthropic.model=${ANTHROPIC_MODEL:claude-3-haiku-20240307}

# Ollama Configuration (local LLM - no API key required)
aem.agent.llm.ollama.base-url=${OLLAMA_BASE_URL:http://localhost:11434}
aem.agent.llm.ollama.model=${OLLAMA_MODEL:llama3.2}
